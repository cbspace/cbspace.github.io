<html>
<head>
  <title>AI News</title>
  <link href="../../images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link rel="stylesheet" href="../../css/main.css">
  <link rel="stylesheet" href="../../css/text_elements.css">
  <link rel="stylesheet" href="../../css/sidepanel.css">
  <script type="text/javascript" src="../../js/mobile.js"></script>
</head>
<body onload="update_for_mobile();">
  <div id="pagecont">
    <div id="sidepanel" class="sidepanel">
        <div id="expandbutton1" class="expandbutton" onclick="sidebar_button_press();"><div class="arrow">&nbsp;&gt;&gt;</div></div>
        <div id="iframecontainer" class="iframecontainer"><iframe id="sidepanelframe" class="sidepanelframe" src="../sidepanel.html"></iframe></div>
        <div id="expandbutton2" class="expandbutton" onclick="sidebar_button_press();"><div class="arrow">&nbsp;&gt;&gt;</div></div>
    </div>
    <div id="maincont" class="maincont">
    <h1>AI News</h1>  
      <div class="linkbold" style="color: #dbd5a1">Differential Transformers</div>
      <div class="subtitle">09/10/2024 6pm AEST</div>
      <p>
      A paper has been released by Microsoft Research, which outlines Differential Transformers, a new method to reduce the noise found in 
      transformer attention blocks. The method makes a simple change to the softmax function by creating two attention heads and subtracting
      the attention scores from eachother. This, in effect, cancels out noise in the attention scores allowing for improved LLM performance.
      <p>
      The <a href="https://arxiv.org/abs/2410.05258" target="_blank">paper</a> outlines how this method can improve contextual understanding in LLMs. 
      It is proposed this method will reduce issues with hallucination and increase in-context learning. The Differential Transformer also requires
      less memory than traditional transformer. There are currenly no models released using this technique but I am sure we will see some soon!

      <div class="linkbold" style="color: #dbd5a1">Meta Release Llama 3.2 Open Weights Models</div>
      <div class="subtitle">26/09/2024 8am AEST</div>
      <p>
      Meta have released their Llama 3.2 series of models which include both small models and multi-modal models. The small models 
      weigh in at 1B and 3B and are text-only models designed for edge devices (phones etc). The larger models are 11B and 90B vision models 
      which are capable of image reasoning.
      <p>
      It's currenly possible to try the smaller models locally using <a href="../blog/running_llms_locally.html" target="_blank">Ollama</a>
      and the larger models will be available soon. To try the 3B you can use:
      <code>ollama run llama3.2</code>. An example of the 11B Vision model processing an image can be seen on my 
      <a href="https://github.com/cbspace/AI-Notebooks/blob/main/Vision%20Models/llama3.2.ipynb" target="_blank">GitHub</a>

      <div class="linkbold" style="color: #dbd5a1">NVIDIA Unveils fVDB to Build Photorealistic 3D World Models</div>
      <div class="subtitle">31/07/2024 7pm AEST</div>
      <p>
      fVDB (Floating-Point Volume/Voxel Data Base) is an extension to the OpenVDB standard which is used to store sparse 
      volumetric data. This new framework allows for large and high resolution 3D scenes to be created and used for deep 
      learning tasks on 3D data. Applications include self driving cars, environment simulations, high resolution 3D image generation 
      and many more.
      <p>
      The <a href="https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/" target="_blank">blog post</a> from NVIDIA
      contains some videos which show realistic 3D city environments creted from spatial data. NVIDIA have also released 
      a <a href="https://arxiv.org/abs/2407.01781" target="_blank">paper</a> which details fVDB.
      
      <div class="linkbold" style="color: #dbd5a1">Meta Release Llama 3.1 Open Weights Models</div>
      <div class="subtitle">24/07/2024 8pm AEST</div>
      <p>
      Meta have released their Llama 3.1 series (referred to as a "herd") of open-weight models and their performance is nothing 
      short of stunning. The largest model has 405B parameters and can outperform GPT 4o in many tasks. Also released were
      8B parameter and 70B parameter models as well as "instruct" versions. The instruct versions are capable of tool usage
      and can be use for AI agents.
      <p>
      Lllama 3.1 models are licenced under a new agreement which allows "distillation" which means you can train smaller models 
      and generate synthetic data using Llama 3.1. The Llama model licence also allows commercial use. The announcement was 
      accompanied by a detailed <a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/" target="_blank">paper</a>
      which outlines the design and training of the models.
      <div id="footer" class="footer" style="margin-top: 40px">Â© Craig Brennan 2024</div>
    </div>
  </div>
</body>
</html>  