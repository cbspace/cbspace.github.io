<html>
<head>
  <title>Activation Functions</title>
  <link href="../../images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link rel="stylesheet" href="../../css/main.css">
  <link rel="stylesheet" href="../../css/text_elements.css">
  <link rel="stylesheet" href="../../css/sidepanel.css">
  <link rel="stylesheet" href="../../css/syntax.css">
  <script type="text/javascript" src="../../js/syntax.js"></script>
  <script type="text/javascript" src="../../js/languages.js"></script>
  <script type="text/javascript" src="../../js/mobile.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body onload="syntax_highlight(); update_for_mobile();">
    <div id="pagecont">
    <div id="sidepanel" class="sidepanel">
        <div id="expandbutton1" class="expandbutton" onclick="sidebar_button_press();"><div class="arrow">&nbsp;&gt;&gt;</div></div>
        <div id="iframecontainer" class="iframecontainer"><iframe id="sidepanelframe" class="sidepanelframe" src="../sidepanel.html"></iframe></div>
        <div id="expandbutton2" class="expandbutton" onclick="sidebar_button_press();"><div class="arrow">&nbsp;&gt;&gt;</div></div>
    </div>
    <div id="maincont" class="maincont">
    <h1>Activation Functions</h1>
    <p>
    Activation functions are used in Neural Networks in order to add a non-linearity to the otherwise linear layers. The addition of a non-linearity 
    ensures that neurons in a Neural Network are separated and not simply stacked linear functions. Without activations the Neural Network would just
    become a linear model as the product of multiple linear functions is a linear function.
    <p>
    The requirements of an activation function are the following:
    <ul>
      <li><b>Non-Linear:</b> In order for the activation function to be effective it must be a non-linear function.</li>
      <li><b>Differentiable:</b> Neural Networks rely on back-propagation in order to "learn" and update their parameters. It is a requirement that 
                                 all elements of a Neural Network are are differentiable in order for back-propagation to work.</li>
    </ul>
    <p>
    The following are the most commonly used activation functions. I have also included the PyTorch functions and methods that are used to implement them.
    <p>

    <h2>Sigmoid</h2>
    <p>
    Sigmoid is a classic activation function which takes a positive or negative x value and outputs a value between 0 and 1. As you can 
    see from the formula below, the sigmoid value when x = 0 is 0.5. Negative x values will give an output of from &gt;0 to &lt;0.5 and positive
    x values will give an output of &gt;0.5 to &lt;1.
    <p>
    <formula>\(\sigma(x) = \frac{1}{1 + e^{-x}} \)</formula>
    <p>
    One useful property of Sigmoid is that very large values will be close to 1 and very small values will be close to 0. Values in the range of about 
    -3 to 3 will have much more resolution is terms of their Sigmoid output compared to larger values. This provides a "squishification" type effect 
    for large or small values.
    <p>
    Sigmoid is typically only used as an output activation function for binary classifiers as it has a marjor drawback when used as an activation 
    between layers within a neural network. The drawback is the "vanishing gradient problem". What this means is that gradients will quickly decay 
    to zero after a few Sigmoid layers due to Sigmoid's small derivative. 

    <p>
    PyTorch Implementation:
<codeblock class="python compact">import torch
from torch import nn 
  
sigmoid = nn.Sigmoid() 
test_value = torch.Tensor([0.0])
print(sigmoid(test_value)</codeblock>
<code>tensor([0.5000])</code>

    <h2>Tanh</h2>
    <p>
    The Tanh function, also known as the hyperbolic tangent, performs in a similar manner to Sigmoid with the key difference bing that it's output range
    is -1 to 1.

    <p>
    <formula>\(\text{Tanh}(x) = \Large{\frac{e^x - e^{-x}}{e^x + e^{-x}}} \)</formula>
    <p>
    
    <h2>ReLU</h2>
    <p>
    The ReLU (Rectified Linear Unit) activation function provides a solution to the vanishing gradient problem. It achieves this by having a much 
    larger derivative than Sigmoid. ReLU is expressed as:
    <p>
    <formula>\( 
      \text{ReLU}(x) = 
      \begin{cases}
        0 & \text{if } x \lt 0 \\  
        x & \text{if } x \geq 0
      \end{cases} 
      \)</formula>
    <p>
    The derivative of ReLU(x) is equal to 1 for positive integers and is equal to 0 for negative integers. Note that the derivative of ReLU is undefined
    for x = 0 however it is acceptable to set the derivative to either 1 or 0 in practical applications. Having a derivative of 1 allows gradients to flow 
    during back-propagation without vanishing.
    <p>
    ReLU is commonly used as an activation function for hidden layers of Neural Networks however there are some downsides to it's use. ReLU has a derivative
    of zero for negative inputs and therefore it's possible for a neuron to become disabled (stuck at zero) if it receives many negative inputs. Furthermore 
    it's also possible to get "exploding gradients" by using ReLU as the output is not bounded. This issue can be mitigated by the use of batch normalisation.
    <p>
    There are many variations of the ReLU activation which address the downsides of ReLU. These include Leaky ReLU, GELU, SwiGLU and many others.

    <h2>Leaky ReLU</h2>
    <p>
    A Leaky ReLU is similar to ReLU but allows for passthrough of negative values. The negative values are defined by a "slope" parameter. Negative values 
    of the input x are multiplied by this slope value (denoted by &alpha;).
    <p>
    <formula>\( 
      \text{ReLU}(x) = 
      \begin{cases}
        \alpha x & \text{if } x \lt 0 \\  
        x & \text{if } x \geq 0
      \end{cases} 
      \)</formula>
    <p>
    Using Leaky ReLU avoids the dying neuron and exploding gradient issues that can occur with standard ReLU. This activation function also introduces another 
    hyperparameter (&alpha;) which can be tuned to increase model performance. 

    <div id="footer" class="footer">Â© Craig Brennan 2024</div>
    </div>
  </div>
</body>
</html>  