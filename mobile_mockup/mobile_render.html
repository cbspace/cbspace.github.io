<html>
<head>
  <title>Mobile Rendering Test</title>
  <link href="../images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link rel="stylesheet" href="../css/main.css">
  <link rel="stylesheet" href="../css/text_elements.css">
  <link rel="stylesheet" href="../css/syntax.css">
  <link rel="stylesheet" href="../css/sidepanel.css">
  <script type="text/javascript" src="../js/syntax.js"></script>
  <script type="text/javascript" src="../js/languages.js"></script>
  <script type="text/javascript" src="../js/mobile.js"></script>
</head>
<body onload="syntax_highlight(); update_for_mobile();">
  <div id="pagecont">
    <div id="sidepanel" class="sidepanel">
        <div id="expandbutton1" class="expandbutton" onclick="sidebar_button_press();"><div class="arrow">&nbsp;&gt;&gt;</div></div>
        <div id="iframecontainer" class="iframecontainer"><iframe id="sidepanelframe" class="sidepanelframe" src="../pages/sidepanel.html"></iframe></div>
        <div id="expandbutton2" class="expandbutton" onclick="sidebar_button_press();"><div class="arrow">&nbsp;&gt;&gt;</div></div>
    </div>
    <div id="maincont" class="maincont">
    <h1>Mobile Rendering Test</h1>
    <p>
    It is a good idea to papers when learning AI <code>coding</code> as they provide good explanations and are free to access.
    Most AI research papers are stored on <a href="https://arxiv.org/" target="_blank">arxiv.org</a> which is an open access platform.
    <p>
    I have listed some popular papers however the list is nowhere near exhaustive. There has been an explosion of new papers
    recently and it can be difficult to keep up with the latest developments. It's still good to read the older papers to
    get an idea of where the latest AI developments have come from. A lot of new papers also build upon previous works.
    <p>
    <h2>Large Language Models</h2>
    <p>
    <a class="linkbold" href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a><br>
    This groundbreaking paper from 2017 introduced the Transformer model. This is what begun the developments of the powerful
    GPT models that we see used in LLMs today.
    <p>
    <a class="linkbold" href="https://arxiv.org/abs/2106.09685" target="_blank">LoRA: Low-Rank Adaptation of Large Language Models</a><br>
    LoRA is a method of fine tuning LLMs without retraining all the parameters. This method instead freezes all the parameters
    and then injects low rank matrices into each layer of the tranformer. These matrices are then fine tuned to the desired
    domain specific content. The result is a significant reduction in compute required for fine tuning with negligilbe loss 
    in model performance.
    <p>
    <a class="linkbold" href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a><br>
    State Space Models are an alternative to Transformers which perform just as well. SSMs have the advantage in terms of 
    compute requirements and scaling. Transformers scale with an N<sup>2</sup> law due to their matrix multiplications whereas Mamba 
    scales linearly due to it's RNN based structure. Mamba adds some additional functionality over basic SSMs and is designed 
    to process natural langauge.
    <p>
<codeblock class="python">
from torch import nn

class BinaryClassifier(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()

        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)
        self.activation = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        x = self.activation(x)
        return x
        
</codeblock>

    <div id="footer" class="footer">Â© Craig Brennan 2024</div>
    </div>
  </div> 
</body>
</html>  